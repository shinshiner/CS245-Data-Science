\documentclass[12pt,a4paper]{article}
\usepackage{ctex}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{enumitem,balance}
\usepackage{wrapfig}
\usepackage{mathrsfs, euscript}
\usepackage[usenames]{xcolor}
\usepackage{hyperref}
\usepackage[vlined,ruled,commentsnumbered,linesnumbered]{algorithm2e}
\usepackage{float}
\usepackage{array}
\usepackage{diagbox}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{aurical}
\usepackage{times}
\usepackage{caption}
\usepackage{fontspec}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\setmainfont{Times New Roman}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{exercise}{Exercise}[section]
\newtheorem*{solution}{Solution}
\theoremstyle{definition}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\postscript}[2]
 {\setlength{\epsfxsize}{#2\hsize}
  \centerline{\epsfbox{#1}}}

\renewcommand{\baselinestretch}{1.05}

\setlength{\oddsidemargin}{-0.365in}
\setlength{\evensidemargin}{-0.365in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{10.1in}
\setlength{\textwidth}{7in}
\makeatletter \renewenvironment{proof}[1][Proof] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
\makeatletter
\renewenvironment{solution}[1][Solution] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

\begin{document}
\noindent
%==========================================================
\noindent\framebox[\linewidth]{\shortstack[c]{
\Large{\emph{关联规则分析与}Apriori\emph{算法}}\vspace{1mm}\\
CS245 \quad 数据科学基础 \quad 陆朝俊 \vspace{1mm} \\
叶泽林 515030910468}}

\section{问题描述}

关联规则分析是数据挖掘中活跃的研究方法之一，其目的是在一个数据集中找出各项之间的关联关系（这种关系一般没有在数据中直接表示出来）。Apriori是关联规则分析中最常用也是最经典的挖掘频繁项集的算法，在本次作业中，我将实现Apriori算法，从交易数据集中发现频繁项集，并生成相应的关联规则。

\vspace{0.01\linewidth}
\section{解决方案\protect\footnote{本次作业的主要代码实现可参见附录 \ref{apd:code}}}

\vspace{0.006\linewidth}
\subsection{数据集的准备}

\vspace{0.006\linewidth}
为验证Apriori算法实现的正确性，我根据一定的规则生成了一个模拟交易数据集，数据集的每个实例代表一条交易记录，共100条记录（在图 \ref{fig:time_com} 对应的实验中会改变数据集大小）。我将商品分为食品（bread、milk、apple、orange、beer），电器（TV、PC、phone、fridge、ele\_oven）和工具（scissors、stapler、plate、knife、glue）三类，各类商品按照一定的出现及组合概率生成相应的交易记录，具体的参数设定可参见附录 \ref{apd:dataset}。

\vspace{0.015\linewidth}
我认为对于交易情况的分析，仅使用模拟生成的数据集难以取得真实的结果，因此我额外在Groceries数据集上执行了Apriori算法。Groceries数据集是内置于R语言的关联分析数据集，来源于某杂货店一个月的真实交易记录，包含9835条交易记录及169种商品。我将其从R语言包中提取出并重组为.csv格式，再使用Apriori算法进行分析。

\vspace{0.006\linewidth}
\subsection{Apriori算法}

\subsubsection{算法描述}
\vspace{0.006\linewidth}
Apriori算法是最经典的挖掘频繁项集的算法，实现了在大数据集上可行的频繁项集挖掘和关联规则提取，其核心思想是\textbf{通过连接产生候选项与其支持度，然后通过剪枝生成频繁项集}，步骤主要为：

\begin{enumerate}

\item 通过自连接获取候选项集，第一次迭代的候选项集即为数据集中的项，之后迭代时的候选项集由前次迭代的频繁项集自连接得出；

\item 每次迭代时的频繁项集通过候选项集剪枝得出，对于候选项集的每条记录，若它或它的某个子集的支持度小于设定的阈值，则被删除；

\item 由频繁项集产生强关联规则（经过上个步骤后满足给定的置信度阈值的关联规则）。

\end{enumerate}

\subsubsection{Python实现细节}

%\vspace{0.01\linewidth}
在这一小节中，我将结合我的部分Python代码对Apriori算法的实现进行介绍。

\vspace{0.01\linewidth}
首先，我将所有频繁1-项集组合起来，称为$L_1$项集，并存储为Python中的集合类型（frozenset类型用于防止误操作更改其中的值）：

\begin{lstlisting}[language=Python,
	numbers=left,
	keywordstyle=\color{blue!70},
	frame=shadowbox,
	breaklines=True]
def _init_data(self):
    deal_list = list()
    item_set = set()

    for d in self.data:
        deal = frozenset(d)
        deal_list.append(deal)

        # 产生 L1 项集
        for item in deal:
            item_set.add(frozenset([item]))

    return item_set, deal_list
\end{lstlisting}

\vspace{0.02\linewidth}
得到$L_1$项集后，我开始迭代求解$L_2...L_k$项集。第i次迭代时，我对$L_i$进行自连接操作，求出i+1项集的集合$C_{i+1}$，自连接的规则为：设$s_1$和$s_2$是$L_i$中的两个频繁i项集，且前i-1个项相同，仅第i项不同，连接$s_1$和$s_2$得到一个i+1项集$C_{i+1}$。Python中通过循环使用并集操作并判断集合大小可以简洁地实现自连接过程：

\vspace{0.02\linewidth}
\begin{lstlisting}[language=Python,
	numbers=left,
	keywordstyle=\color{blue!70},
	frame=shadowbox,
	breaklines=True]
set([i.union(j) for i in item_set for j in item_set if len(i.union(j)) == id])
\end{lstlisting}

\vspace{0.02\linewidth}
自连接构造出$C_{i+1}$后，我通过扫描交易数据集检测出$C_{i+1}$中的频繁i+1项集，得到$L_{i+1}$，此处存在一个优化点：若有某元素的子集不在$L_i$中即可删除。

\vspace{0.02\linewidth}
\begin{lstlisting}[language=Python,
	numbers=left,
	keywordstyle=\color{blue!70},
	frame=shadowbox,
	breaklines=True]
# 根据支持度阈值删除项集
    def _remove_item_set(self, item_set, deals, freq_set):
        res = set()
        local_set = defaultdict(int)

        # 统计
        for item in item_set:
            for d in deals:
                if item.issubset(d):
                    freq_set[item] += 1
                    local_set[item] += 1

        # 删除
        for item, count in local_set.items():
            support = float(count) / len(deals)
            if support >= self.sup:
                res.add(item)

        return res
\end{lstlisting}

\vspace{0.01\linewidth}
将上述迭代过程不断重复，直到项集大小达到商品个数，迭代结束，得到最终结果。

\vspace{0.01\linewidth}
为验证Apriori算法实现的正确性，我先使用蛮力算法在模拟交易数据集上运行一次，将结果与Apriori算法的结果进行比较。验证算法的正确性后，在Groceries数据集上我则直接使用Apriori算法进行关联规则分析。


\section{实验及结果}

\subsection{模拟数据集}

我将支持度和置信度的阈值分别设置为0.1及0.6，蛮力算法的运行结果为：

\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{蛮力算法在模拟交易数据集下发现的频繁项集}
	\label{tab:baoli_sim_sup}
	\centering
	
	\begin{tabular}{c|c|c|c}
		\centering
		频繁项集 & 支持度 & 频繁项集 & 支持度 \\
		\hline
		scissors & 0.25 & apple & 0.18 \\
		beer & 0.22 & fridge & 0.25 \\
		ele\_oven & 0.26 & stapler & 0.30 \\
		phone & 0.22 & glue & 0.23 \\
		plate & 0.23 & scissors, knife & 0.15 \\
		PC & 0.22 & ele\_oven, TV & 0.17 \\
		TV & 0.29 & scissors, stapler & 0.15 \\
		bread & 0.19 & glue, stapler & 0.16 \\
		orange & 0.21 & stapler, plate & 0.15 \\
		knife & 0.27 & stapler, knife & 0.15 \\
		milk & 0.18 & & \\		
	\end{tabular}
\end{table}

\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{蛮力算法在模拟交易数据集下发现的关联规则}
	\label{tab:baoli_sim_con}
	\centering
	
	\begin{tabular}{c|c}
		\centering
		关联规则 & 置信度 \\
		\hline
		glue --> stapler & 0.70 \\
		scissors --> knife & 0.60 \\
		ele\_oven --> TV & 0.65 \\
		plate --> stapler & 0.65 \\
		scissors --> stapler & 0.60 \\
	\end{tabular}
\end{table}

得到以上的参考结果后，我使用自己实现的Apriori算法和同样的参数（支持度0.1，置信度0.6）对模拟交易数据集进行分析，所得结果见表 \ref{tab:apriori_sim_sup} 和表 \ref{tab:apriori_sim_con}。

\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{Apriori算法在模拟交易数据集下发现的关联规则（按置信度排序）}
	\label{tab:apriori_sim_con}
	\centering
	
	\begin{tabular}{c|c}
		\centering
		关联规则 & 置信度 \\
		\hline
		scissors --> stapler & 0.60 \\
		scissors --> knife & 0.60 \\
		plate --> stapler & 0.65 \\
		ele\_oven --> TV & 0.65 \\
		glue --> stapler & 0.70 \\
	\end{tabular}
\end{table}

\vspace{-0.03\linewidth}
\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{Apriori算法在模拟交易数据集下发现的频繁项集（按支持度排序）}
	\label{tab:apriori_sim_sup}
	\centering
	
	\begin{tabular}{c|c|c|c}
		\centering
		频繁项集 & 支持度 & 频繁项集 & 支持度 \\
		\hline
		stapler, scissors & 0.15 & PC & 0.22 \\
		knife, scissors & 0.15 & phone & 0.22 \\
		stapler, knife & 0.15 & glue & 0.23 \\
		plate, stapler & 0.15 &plate & 0.23 \\
		stapler, glue & 0.16 & scissors & 0.25 \\
		ele\_oven, TV & 0.17 & fridge & 0.25 \\
		milk & 0.18 & ele\_oven & 0.26 \\
		apple & 0.18 & knife & 0.27 \\
		bread & 0.19 & TV & 0.29 \\
		orange & 0.21 & stapler & 0.30 \\
		beer & 0.22 & & \\		
	\end{tabular}
\end{table}

\vspace{-0.02\linewidth}
将表 \ref{tab:baoli_sim_sup}， \ref{tab:baoli_sim_con}的结果排序后与上表对比，容易发现二者一致。考虑到模拟交易数据集在生成时的随机性，这个结果可以证明我实现的Apriori算法的正确性。

为探索该数据集中所有项集和相应关联规则的分布情况，我将支持度和置信度的阈值设为0，运行Apriori算法，并作出项集分布的热度图以及关联规则置信度的核密度图，结果可参见图 \ref{fig:dis}（热度图中编号从1至15的商品分别为bread、milk、apple、orange、beer、TV、PC、phone、fridge、ele\_oven、scissors、stapler、plate、knife、glue；作核密度图时我采用高斯核）。

\vspace{-0.012\linewidth}
\begin{figure}[H]
	\centering
	\subfigure[项集]{
		\label{fig:dis-a}
		\includegraphics[width=0.40\linewidth]{img/heatmap_fre.png}
	}
	\subfigure[关联规则]{
		\label{fig:dis-b}
		\includegraphics[width=0.40\linewidth]{img/prob_density_rule.png}
	}
	\caption{模拟交易数据集中项集和相应关联规则的分布（由于之前计算出的频繁项集最多包含两项商品，因此此处只作出二维热度图）}
	\label{fig:dis}
\end{figure}

从图 \ref{fig:dis-a}中容易看出，支持度相对高的项集集中于编号为1，6，11的商品处，分别为（bread，TV，scissors），并且支持度高的点集中于对角线附近，表明仅有一个元素的项集有着更高的支持度。图 \ref{fig:dis-b}反映出关联规则对应的置信度普遍偏低（集中于0-0.4，较大部分在0.1以下），虽有部分达到0.5的置信度，但置信度稍高（0.6以上）的关联规则仅有表 \ref{tab:apriori_sim_con}中所列的数个。

\vspace{0.01\linewidth}
根据理论分析，Apriori算法相对于蛮力算法有着极大的效率优势，为直观地探索这种优势，我在不同的交易记录数下运行两种算法进行频繁项集挖掘，对比消耗的时间，结果参见图 \ref{fig:time_com}。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{img/time_kline.png}
	\caption{Apriori算法与蛮力算法进行频繁项集挖掘效率对比}
	\label{fig:time_com}
\end{figure}

从图中可以看出，在同样的数据规模下，Apriori算法消耗的时间远少于蛮力算法，上图的比较仅改变了数据集的交易记录数量，若增加总交易货品种类，蛮力算法的复杂度将呈指数型增长，Apriori算法的效率优势会更加明显（限于计算资源，本次作业中暂不进行不同商品类数下的效率比较）。

\vspace{0.01\linewidth}
\subsection{Groceries数据集}

\vspace{0.01\linewidth}
在模拟交易数据集上确认Apriori算法实现的正确性后，我将算法应用到真实数据上。考虑到Groceries数据集中商品种类相对于实例较多的特点，即每种商品出现的次数占总交易记录数的比例可能偏低，我选择了较小的支持度（0.05），置信度选择为0.2。运行结果参见表 \ref{tab:apriori_gro_sup} 和表 \ref{tab:apriori_gro_con}。

\vspace{0.01\linewidth}
\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{Apriori算法在Groceries数据集下发现的关联规则（按置信度排序）}
	\label{tab:apriori_gro_con}
	\centering
	
	\begin{tabular}{c|c}
		\centering
		关联规则 & 置信度 \\
		\hline
		whole milk --> yogurt & 0.22 \\
		whole milk --> rolls/buns & 0.22 \\
		whole milk --> other vegetables & 0.29 \\
		rolls/buns --> whole milk & 0.31 \\
		other vegetables --> whole milk & 0.39 \\
		yogurt --> whole milk & 0.40 \\
	\end{tabular}
\end{table}

\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{Apriori算法在Groceries数据集下发现的频繁项集（按支持度排序）}
	\label{tab:apriori_gro_sup}
	\centering
	
	\begin{tabular}{c|c|c|c}
		\centering
		频繁项集 & 支持度 & 频繁项集 & 支持度 \\
		\hline
		napkins & 0.05 & canned beer & 0.08 \\
		beef & 0.05 & newspapers & 0.08 \\
		curd & 0.05 & bottled beer & 0.08 \\
		butter & 0.06 & citrus fruit & 0.08 \\
		yogurt, whole milk & 0.06 & pastry & 0.09 \\
		rolls/buns, whole milk & 0.06 & sausage & 0.09 \\
		pork & 0.06 & shopping bags & 0.10 \\
		coffee & 0.06 & tropical fruit & 0.10 \\
		margarine & 0.06 & root vegetables & 0.11 \\
		frankfurter & 0.06 & bottled water & 0.11 \\
		domestic eggs & 0.06 & yogurt & 0.14 \\
		brown bread & 0.06 & soda & 0.17 \\
		whipped/sour cream & 0.07 & rolls/buns & 0.18 \\
		fruit/vegetable juice & 0.07 & other vegetables & 0.19 \\
		other vegetables, whole milk & 0.07 & whole milk & 0.26 \\
		pip fruit & 0.08 & &
	\end{tabular}
\end{table}

结果表明，频繁项集集中在大小为1或2的集合中。为进一步探索Groceries数据集的关联规则，我在不同的支持度和置信度下使用Apriori算法对其进行关联规则分析，并将结果可视化，可参见图 \ref{fig:apriori}。

%\vspace{-0.012\linewidth}
\begin{figure}[H]
	\centering
	\subfigure[频繁项集个数]{
		\includegraphics[width=0.475\linewidth]{img/gro_item_bar.png}
	}
	\subfigure[关联规则个数]{
		\includegraphics[width=0.475\linewidth]{img/gro_rule_kline.png}
	}
	\caption{频繁项集个数及关联规则个数与支持度和置信度关系}
	\label{fig:apriori}
\end{figure}

结果表明，在支持度大于0.1时，Groceries数据集中基本已找不出满足条件的频繁项集；而置信度大于0.5时，几乎无法在剩余的频繁项集中发现关联规则。这个结果进一步证明了实验前的假设：每种商品出现的次数占总交易记录的比例偏低。同时，从置信度和关联规则个数的关系分析，Groceries数据集中的交易记录存在一定的关联规则，但相应的置信度普遍偏低（0.5以下）。

\section{结论}

本次作业中，我利用Python语言实现了Apriori算法，并通过在模拟交易数据集上与蛮力算法的结果比较，证明了算法实现的正确性。之后我使用Apriori算法对模拟交易数据集以及Groceries数据集进行了频繁项集挖掘，进而分析关联规则。

\vspace{0.01\linewidth}
结果表明，Apriori算法是迅速且有效的挖掘频繁项集的算法，在中小规模的数据集上有着良好的表现。限于计算资源，我没有进行大规模数据上的实验，但从理论上分析，Apriori算法在大规模数据上的应用仍存在多个优化点，如对数据集的扫描次数，对候选项集支持度的计数方法等。

\newpage
\begin{appendix}
	\section{附录}
	\subsection{模拟交易数据集的详细信息}
	\label{apd:dataset}
	
	我将模拟交易数据集的交易记录按照交易商品数分为4类，即2、3、4、5件。不同的商品件数按照不同的比例随机混合三类商品，具体混合规则可参见表 \ref{tab:dataset}。
	
	\begin{table}[H]
		\renewcommand\arraystretch{1.35}
		\caption{模拟交易数据集生成交易记录的混合规则}
		\label{tab:dataset}
		\centering
		
		\begin{tabular}{c|c}
			\centering
			商品数 & 混合规则（括号中数字代表各类商品在记录中所占数量） \\
			\hline
			2 & (2); (1, 1) \\
			3 & (3); (2, 1) \\
			4 & (4); (3, 1); (2, 1, 1) \\
			5 & (5); (4, 1); (3, 2); (2, 2, 1) \\		
		\end{tabular}
	\end{table}
	
	
	\subsection{Apriori算法实现代码}
	\label{apd:code}
	
	\begin{lstlisting}[language=Python,
	numbers=left,
	keywordstyle=\color{blue!70},
	frame=shadowbox,
	breaklines=True]
from itertools import chain, combinations
from collections import defaultdict

class Apriori(object):
    def __init__(self, f_name, sup=0.1, con=0.1):
        self.data = self._read_csv(f_name)
        self.sup = sup
        self.con = con
        self.items = []
        self.rules = []

    def _read_csv(self, f_name):
        with open(f_name, 'r') as f:
            for line in f:
                line = line.strip().rstrip(',')
                item = frozenset(line.split(','))
                yield item

    def run(self):
        # 小工具函数
        def _get_support(item):
            return float(freq_set[item]) / len(deals)
        def _get_subsets(item):
            return chain(*[combinations(item, i + 1) for i, a in enumerate(item)])
        def _join_set(item_set, id):
            return set([i.union(j) for i in item_set for j in item_set if len(i.union(j)) == id])

        # 初始化 L1 项集和交易数据
        item_set, deals = self._init_data()

        freq_set = defaultdict(int)
        large_set = dict()

        init_set = self._remove_item_set(item_set, deals, freq_set)
        l_set = init_set

        k = 2
        while (l_set != set([])):
            large_set[k - 1] = l_set
            l_set = _join_set(l_set, k)
            c_set = self._remove_item_set(l_set, deals, freq_set)
            l_set = c_set
            k += 1

        # 组合结果
        for key, value in large_set.items():
            self.items.extend([(tuple(item), _get_support(item)) for item in value])

        for key, value in list(large_set.items())[1:]:
            for item in value:
                _subsets = map(frozenset, [x for x in _get_subsets(item)])
                for element in _subsets:
                    remain = item.difference(element)
                    if len(remain) > 0:
                        con = _get_support(item) / _get_support(element)
                        if con >= self.con:
                            self.rules.append(((tuple(element), tuple(remain)), con))

    def _init_data(self):
        deal_list = list()
        item_set = set()

        for d in self.data:
            deal = frozenset(d)
            deal_list.append(deal)

            # 产生 L1 项集
            for item in deal:
                item_set.add(frozenset([item]))

        return item_set, deal_list

    # 根据支持度阈值删除项集
    def _remove_item_set(self, item_set, deals, freq_set):
        res = set()
        local_set = defaultdict(int)

        # 统计
        for item in item_set:
            for d in deals:
                if item.issubset(d):
                    freq_set[item] += 1
                    local_set[item] += 1

        # 删除
        for item, count in local_set.items():
            support = float(count) / len(deals)
            if support >= self.sup:
                res.add(item)

        return res

    # 输出结果
    def show(self):
        print(u'------频繁项集------')
        for item, sup in sorted(self.items, key=lambda items: items[1]):
            print ('%s , %.2f' % (str(item), sup))
        print (u'\n------关联规则------')
        for rule, con in sorted(self.rules, key=lambda rules: rules[1]):
            pre, post = rule
            print ('%s --> %s , %.2f' % (str(pre), str(post), con))

if __name__ == "__main__":
    a = Apriori('groceries.csv', 0.05, 0.2)
    a.run()
    a.show()
	\end{lstlisting}
	
\end{appendix}

%========================================================================
\end{document}